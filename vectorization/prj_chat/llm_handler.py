# llm_handler.py
import time
from typing import Optional
from ollama import Client

class LLMHandler:
    def __init__(self, 
                 emb_model, 
                 faiss_manager, 
                 model_name: str = 'llama3.2',
                 temperature: float = 0.3,
                 max_tokens: int = 100):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ LLM
        
        :param emb_model: –ú–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        :param faiss_manager: –ú–µ–Ω–µ–¥–∂–µ—Ä –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        :param model_name: –ù–∞–∑–≤–∞–Ω–∏–µ LLM –º–æ–¥–µ–ª–∏
        :param temperature: –ü–∞—Ä–∞–º–µ—Ç—Ä —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        :param max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ
        """
        self.emb_model = emb_model
        self.faiss = faiss_manager
        self.client = Client(host='http://localhost:11434')
        
        self.model_name = model_name
        self.temperature = temperature
        self.max_tokens = max_tokens
        
        print("\n[LLMHandler] Initializing language model handler")
        print(f"  ‚Ä¢ Model: {self.model_name}")
        print(f"  ‚Ä¢ Temperature: {self.temperature}")
        print(f"  ‚Ä¢ Max tokens: {self.max_tokens}\n")

    def generate_response(self, question: str) -> str:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ —Å –ø–æ–ª–Ω—ã–º —Ü–∏–∫–ª–æ–º RAG
        
        :param question: –í—Ö–æ–¥–Ω–æ–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        :return: –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
        """
        try:
            start_time = time.time()
            
            # –≠—Ç–∞–ø 1: –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            print(f"[RAG Pipeline] Processing question: '{question}'")
            context = self._get_context(question)
            
            if not context:
                return "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫–æ–¥–æ–≤–æ–π –±–∞–∑–µ"
            
            # –≠—Ç–∞–ø 2: –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞
            prompt = self._build_prompt(question, context)
            
            # –≠—Ç–∞–ø 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            print("[Generation] Sending request to LLM...")
            response = self.client.generate(
                model=self.model_name,
                prompt=prompt,
                options={
                    'temperature': self.temperature,
                    'num_predict': self.max_tokens
                }
            )
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            print(f"[Generation] Response received ({time.time()-start_time:.1f}s)")
            print(f"  ‚Ä¢ Tokens used: {response.get('eval_count', 'N/A')}")
            
            return response['response']
            
        except Exception as e:
            print(f"[Error] Failed to generate response: {str(e)}")
            return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}"

    def _get_context(self, question: str) -> Optional[str]:
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ"""
        try:
            print("[Retrieval] Encoding question...")
            start_time = time.time()
            
            question_emb = self.emb_model.encode([question])
            print(f"  ‚úì Question encoded ({time.time()-start_time:.2f}s)")
            
            print("[Retrieval] Searching in vector database...")
            distances, indices = self.faiss.index.search(question_emb, 3)
            
            valid_indices = [i for i in indices[0] if 0 <= i < len(self.faiss.metadata)]
            if not valid_indices:
                print("  ‚úó No relevant context found")
                return None
                
            print(f"  ‚úì Found {len(valid_indices)} relevant chunks")
            return "\n\n".join(
                f"File: {self.faiss.metadata[i]['path']}\nContent:\n{self.faiss.metadata[i]['content']}"
                for i in valid_indices
            )
            
        except Exception as e:
            print(f"[Retrieval Error] {str(e)}")
            return None

    def _build_prompt(self, question: str, context: str) -> str:
        """–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è LLM"""
        print("[Prompt Engineering] Building prompt...")
        
        prompt_template = (
            "Context from codebase:\n{context}\n\n"
            "Answer the question based on the context above.\n"
            "Question: {question}\n"
            "Answer (be concise, 2-3 sentences):"
        )
        
        return prompt_template.format(context=context, question=question)

    def interactive_loop(self):
        """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç"""
        print("\n=== Interactive Mode ===")
        while True:
            try:
                question = input("\n‚ùì Your question (type 'exit' to quit): ").strip()
                if question.lower() in ('exit', 'quit'):
                    break
                
                start_time = time.time()
                print("\nüîç Processing...")
                response = self.generate_response(question)
                
                print(f"\n‚è± Response time: {time.time()-start_time:.1f}s")
                print(f"üí° Answer:\n{'-'*40}\n{response}\n{'-'*40}")
                
            except KeyboardInterrupt:
                print("\nüõë Operation cancelled by user")
                break
                
        print("\n=== Session Ended ===")