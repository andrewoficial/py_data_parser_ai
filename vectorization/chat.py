import sys
import time
import json
import gc
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from hashlib import md5
from ollama import Client
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

class OptimizedCppRAG:
    def __init__(self):
        self.ollama = Client(host='http://localhost:11434')
        self.emb_model = None
        self.index = None
        self.metadata = []
        self.ignore_dirs = {'build', 'cmake-build', 'thirdparty', 'test', 'docs'}
        self.chunk_size = 256
        self.overlap = 32
        self.base_dir = Path(r'D:\AI\RagLoangGas')
        self.index_path = self.base_dir / 'vector_store'
        self.cache_path = self.base_dir / 'file_hashes.json'
        self._init_embedding_model()

    def _init_embedding_model(self):
        if self.emb_model is not None:
            del self.emb_model
            gc.collect()
        self.emb_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')

    def _collect_files(self, path: Path):
        extensions = {'cpp', 'hpp', 'h', 'cc', 'cxx', 'c', 'inl'}
        for file_path in path.rglob('*'):
            if file_path.suffix[1:] not in extensions:
                continue
            if any(part in self.ignore_dirs for part in file_path.parts):
                continue
            if file_path.is_file() and file_path.stat().st_size < 2 * 1024 * 1024:
                yield file_path

    # –î–æ–±–∞–≤–ª–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã
    def _load_existing_index(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∏–Ω–¥–µ–∫—Å–∞"""
        if (self.index_path / 'index.faiss').exists():
            try:
                self.index = faiss.read_index(str(self.index_path / 'index.faiss'))
                with open(self.index_path / 'metadata.json', 'r') as f:
                    self.metadata = json.load(f)
                return True
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞: {str(e)}")
        return False

    def _save_index(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞"""
        self.index_path.mkdir(exist_ok=True)
        faiss.write_index(self.index, str(self.index_path / 'index.faiss'))
        with open(self.index_path / 'metadata.json', 'w') as f:
            json.dump(self.metadata, f)

    def _get_file_hashes(self, files):
        return {str(f): md5(f.read_bytes()).hexdigest() for f in files}

    def _save_file_hashes(self, hashes):
        with open(self.cache_path, 'w') as f:
            json.dump(hashes, f)

    def _get_changed_files(self, current_hashes):
        try:
            with open(self.cache_path, 'r') as f:
                old_hashes = json.load(f)
            return [Path(f) for f, h in current_hashes.items() if old_hashes.get(f) != h]
        except FileNotFoundError:
            return [Path(f) for f in current_hashes.keys()]

    def _print_progress(self, processed: int, total: int):
        progress = processed / total
        bar = '#' * int(progress * 20)
        sys.stdout.write(f"\r[{bar.ljust(20)}] {processed}/{total} ({progress:.1%})")
        sys.stdout.flush()

    def index_codebase(self):
        if self._load_existing_index():
            print("–ó–∞–≥—Ä—É–∂–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å")
            return

        source_dir = self.base_dir / 'sources'
        files = list(self._collect_files(source_dir))
        if not files:
            print("–§–∞–π–ª—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return

        file_hashes = self._get_file_hashes(files)
        changed_files = self._get_changed_files(file_hashes)

        if not changed_files:
            print("–ò–Ω–¥–µ–∫—Å –∞–∫—Ç—É–∞–ª–µ–Ω, –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")
            return

        print(f"–ù–∞–π–¥–µ–Ω–æ {len(changed_files)} —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏...")
        self.index = faiss.IndexFlatIP(384)
        
        start_time = time.time()
        with ThreadPoolExecutor(max_workers=2) as executor:
            futures = []
            for i, file in enumerate(changed_files, 1):
                futures.append(executor.submit(self._process_file, file, i, len(changed_files)))
                if i % 5 == 0:
                    self._free_memory()

            for i, future in enumerate(futures, 1):
                try:
                    future.result()
                    self._print_progress(i, len(changed_files))
                except Exception as e:
                    print(f"\n–û—à–∏–±–∫–∞: {str(e)}")

        self._save_index()
        self._save_file_hashes(file_hashes)
        print(f"\n–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {time.time()-start_time:.1f} —Å–µ–∫.")
        print(f"–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(self.metadata)}")

    def _smart_chunking(self, code: str) -> list:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏"""
        if not code or not code.strip():
            return []
            
        lines = code.split('\n')
        chunks = []
        pos = 0
        max_lines = 5000  # –ú–∞–∫—Å–∏–º—É–º 5000 —Å—Ç—Ä–æ–∫ –Ω–∞ —Ñ–∞–π–ª
        
        while pos < len(lines) and pos < max_lines:
            end = min(pos + self.chunk_size, len(lines), max_lines)
            chunk = lines[pos:end]
            
            # –ü–æ–∏—Å–∫ –≥—Ä–∞–Ω–∏—Ü —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞
            boundary = self._find_logical_boundary(chunk)
            if boundary > 0 and boundary < len(chunk):
                actual_end = pos + boundary
                chunk = lines[pos:actual_end]
                pos = actual_end - self.overlap
            else:
                pos = end - self.overlap
                
            if chunk:  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–ø—É—Å—Ç—ã–µ —á–∞–Ω–∫–∏
                chunks.append('\n'.join(chunk))
            
            if pos < 0:
                break

        return chunks

    def _find_logical_boundary(self, chunk: list) -> int:
        """–ü–æ–∏—Å–∫ –≥—Ä–∞–Ω–∏—Ü –±–ª–æ–∫–æ–≤ –∫–æ–¥–∞"""
        stack = []
        for i, line in enumerate(chunk):
            stripped = line.strip()
            if stripped.startswith(('class ', 'struct ', 'namespace ', 'void ', 'int ', 'bool ')):
                if not stack:
                    return i
            if '{' in line:
                stack.append(i)
            if '}' in line:
                if stack:
                    stack.pop()
                    if not stack:
                        return i+1
        return 0


    def _free_memory(self):
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏"""
        if self.emb_model is not None:
            del self.emb_model
            self.emb_model = None
        gc.collect()
        time.sleep(0.5)  # –î–∞–µ–º –≤—Ä–µ–º—è –Ω–∞ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏



    def _process_file(self, file_path: Path, file_num: int, total_files: int):
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞ –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
            if file_path.stat().st_size > 1 * 1024 * 1024:  # 1MB
                print(f"‚ö†Ô∏è –§–∞–π–ª {file_path.name} —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                return

            print(f"\nüìÅ –û–±—Ä–∞–±–æ—Ç–∫–∞ ({file_num}/{total_files}): {file_path.name}")
            
            with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                content = f.read()
                chunks = self._smart_chunking(content)
                
                if not chunks or len(chunks) == 0:
                    print(f"‚ö†Ô∏è –§–∞–π–ª {file_path.name} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
                    return
                
                batch_size = 5  # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
                for i in range(0, len(chunks), batch_size):
                    batch = chunks[i:i+batch_size]
                    
                    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø—É—Å—Ç—ã—Ö —á–∞–Ω–∫–æ–≤
                    valid_chunks = [chunk for chunk in batch if chunk.strip()]
                    if not valid_chunks:
                        continue
                    
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –ø–∞–º—è—Ç–∏
                    try:
                        embs = self.emb_model.encode(valid_chunks, convert_to_numpy=True)
                        self.index.add(embs)
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤
                        self.metadata.extend({
                            'path': str(file_path),
                            'content': chunk[:100],
                            'hash': md5(chunk.encode()).hexdigest()
                        } for chunk in valid_chunks)
                        
                    except Exception as e:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞: {str(e)}")
                    finally:
                        del embs, valid_chunks, batch
                        gc.collect()

        except Exception as e:
            print(f"‚ö†Ô∏è –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ {file_path.name}: {str(e)}")
        finally:
            del content, chunks
            gc.collect()


    def ask(self, question: str) -> str:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞"""
        try:
            question_emb = self.emb_model.encode([question])
            distances, indices = self.index.search(question_emb, 2)
            
            # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –∏–Ω–¥–µ–∫—Å–æ–≤
            valid_indices = [i for i in indices[0] if 0 <= i < len(self.metadata)]
            
            if not valid_indices:
                return "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
                
            context = "\n".join(
                f"[[–§–∞–π–ª: {self.metadata[i]['path']}]]\n{self.metadata[i]['content']}"
                for i in valid_indices
            )
        
            prompt = (
                "–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n\n"
                "–í–æ–ø—Ä–æ—Å: {question}\n"
                "–û—Ç–≤–µ—Ç (–∫—Ä–∞—Ç–∫–æ, 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è):"
            ).format(context=context, question=question)

            response = self.ollama.generate(
                model='llama3.2',
                prompt=prompt,
                options={'temperature': 0.3, 'num_predict': 100}
            )
            return response['response']
        
        except Exception as e:
            return f"–û—à–∏–±–∫–∞: {str(e)}"


if __name__ == "__main__":
    try:
        rag = OptimizedCppRAG()
        print("üîÑ –ù–∞—á–∞–ª–æ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...")
        rag.index_codebase()
        
        # –î–æ–±–∞–≤–∏—Ç—å —ç—Ç–æ—Ç –±–ª–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
        while True:
            question = input("\n‚ùì –í–æ–ø—Ä–æ—Å (–∏–ª–∏ 'exit'): ").strip()
            if question.lower() in ('exit', 'quit'):
                break
                
            start = time.time()
            print("\nüîç –ü–æ–∏—Å–∫...")
            response = rag.ask(question)  # –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –º–µ—Ç–æ–¥ ask() —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            print(f"\n‚è± {time.time()-start:.1f} —Å–µ–∫.\nüí° –û—Ç–≤–µ—Ç:\n{response}")
            
    except KeyboardInterrupt:
        print("\nüõë –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...")
    finally:
        rag._free_memory()
        gc.collect()